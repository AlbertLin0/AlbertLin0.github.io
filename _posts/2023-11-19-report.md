---
title: report
author: Haibo_Hu
date: 2023-11-04 11:33:00 +0800
categories: [Report]
tags: [读书笔记]
math: true
---

## 大模型的压缩能力

#### 1. **大模型的训练思维与压缩类似**
   
很多人相信压缩即智能。2月28日，OpenAI 的核心研发人员 Jack Rae 在参加 Stanford MLSys Seminar 的访谈时进行了一个名为 Compression for AGI 的主题分享，其核心观点为：AGI 基础模型的目标是实现对**有效信息最大限度的无损压缩。** 其主要思想如下：

##### 直观理解

![Alt text](/assets/img/image.png){: w="700" h="400" }

从查表学习，压缩后的数据能够得到本质的规律，比如苹果掉下来，可以联系到重力。OpenAI认为压缩可以通往通用人工智能。

#### 2. **训练方法与算术编码类似**

![Alt text](/assets/img/image-2.png)

其本质是找到最优的预测下一个token的概率分布，
![Alt text](/assets/img/image-1.png){: w="700" h="400" }

#### 3. **小实验证明其压缩能力**
引用自[Semantic Compression With Large Language Models](https://arxiv.org/abs/2304.12512)

![Alt text](/assets/img/image-3.png)

![Alt text](/assets/img/image-4.png)


语义相似性：

![Alt text](/assets/img/image-5.png)

#### 4. **缺点**

1. 压缩一切的思维很疯狂。
2. 对于大分辨率的图像来说，很难去压缩它，应该考虑传统的方法，比如压缩感知，即选择其有效的输入，降低图像的稀疏性。从概率的角度上讲，即降低图像的熵，让预测下一个像素的概率分布更平滑。
3. 很多数据观测不到，需要一些meta信息。


## 对于图像 